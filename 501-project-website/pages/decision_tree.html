<!DOCTYPE html>
<html>

	<head>
		<title>Decision Tree</title>
		<link rel="stylesheet" href="../styles.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
		<script>hljs.initHighlightingOnLoad();</script>
	</head>

	<body>
		
	<!-- HEADER BAR -->
	<ul>
	    <!-- link back to homepage -->
	    <li><a href="../index.html">About me</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="https://github.com/anly501/anly-501-project-hx33/tree/main/codes">Code</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="https://github.com/anly501/anly-501-project-hx33/tree/main/data">Data</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="./portfolio_introduction.html">Introduction</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="./data_gathering.html">Data Gathering</a></li>

	    <!-- tab with dropdown -->
	    <li><a href="./data_cleaning.html">Data Cleaning</a></li>

	    <!-- tab with dropdown -->
	    <li><a href="./exploring_data.html">Exploring Data</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="./naive_bayes.html">Naive Bayes</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="./decision_tree.html">Decision Tree</a></li>

	    <!-- tab without dropdown  -->
	    <li><a href="./SVM.html">SVM</a></li>
		
		<!-- tab without dropdown  -->
	    <li><a href="./Clustering.html">Clustering</a></li>

	    <!-- <li class="dropdown">
		<a href="javascript:void(0)" class="dropbtn">local reference</a>
		<div class="dropdown-content">
		<a href="./pages/html-cheatsheet/cheatsheet.html">html</a>
		<a href="./pages/markdown-cheatsheet/markdown-cheatsheet.html">markdown</a>
		</div>
	    </li> -->  
	</ul>

	<div id="id-of-element-to-link-to">

	    <h1  id="C1"><center>Decision Tree<center/></h1>
			<p style="font-size:18px;">
			Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. Like Naive Bayes, the goal is 
				to create a tree-like model that predicts the value of a target variable (y) by learning simple decision rules inferred from the data features (X).
				The DT classidier model is capable of both binary and multiclass classification, and in this project I will used a binary target variable (-1,1).
			</p> 
			
			<p style="font-size:18px;">
			Decision Tree algorithm has the advantage that its trees can be visualized, which means that the decision steps can be viewed and are 
				simple to interpret. The models are also easy to validate statistical tests to increase the reliability of the model. However, sometimes 
				the models can often be overfitting and create over-complex trees. Therefore, setting a maximum depth of the tree are usually necessary.	
			</p> 
			
		<h2  id="C1"><center>Students Enrollment 2015-2020 Dataset<center/></h2>
			
			<h3  id="C1">Class distribution</h3>
			<p style="font-size:18px;">
			The goal of using Decision Tree on this dataset is to check whether some variables (grades, races or poverty issues) are related to the difference 
				between the enrollments of girls and boys. To do this the labels that represent the difference should be created as the class variable y.
				Therefore, I choose create 4 class labels: the schools with girls enrollments that's less than half of total enrollments will be labeled as 0, 
				with girls enrollments above 50% labeled as 1. 
			</p>
			
			<pre><code class="python">
				df = pd.read_csv("cleaned_2016_2020_enroll.csv")
				y = [0]*len(df)
				for i in range(len(df)):
					prop = df.loc[i,"X..Female"]/df.loc[i,"Total.Enrollment"]
					if prop < 0.5:
						y[i] = 0
					if prop > 0.5:
						y[i] = 1

				df["y"] = y
			</code> </pre>
			
			
			<h3  id="C1">Baseline model for comparison</h3>
			<p style="font-size:18px;">
			As a baseline comparison, a random classifier on the data was performed. This step carried out a uniform random number generation with random 
				numbers sampled based on the distribution of labels in dataset (y labels). The reported accuracy of the random classifier is 0.34, with 
				recall values 0.65 and f-score 0.37.
			</p>
			
			<pre><code class="python">
				def random_classifier(y_data):
					ypred=[];
					max_label=np.max(y_data); #print(max_label)
					for i in range(0,len(y_data)):
						ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))
					print("-----RANDOM CLASSIFIER-----")
					print("count of prediction:",Counter(ypred).values())
					print("probability of prediction:",np.fromiter(Counter(ypred).values(),dtype=float)/len(y_data))
					print("accuracy",accuracy_score(y_data, ypred))
					print("percision, recall, fscore,",precision_recall_fscore_support(y_data,ypred))
				df1 = df
				random_classifier(df1["y"])
			</code> </pre>
			
			
			<h3  id="C1">Feature selection</h3>
			
			<p style="font-size:18px;">
			Then, the decision tree model will use this label as y, and the feature vectors X include either <br>
				1. enrollments for different grades in either elementary, middle or high school, <br>
				2. enrollments for different races. <br>
				
				The dataset will be separated it into training and testing sets. DT will build the classification model after training on the training data, 
				and will make predictions on test data. Then, appropriate visualizations and result summaries will be drawn based on the comparison of the
				predicted labels and the real labels. 
			</p>
			
			<pre><code class="python">
				mid = df1.loc[df1["mid"]==1,"Grade.6":"Grade.8"]
				mid["y"] = df1.loc[df1["mid"]==1,"y"]
				mid["Total"] = df1.loc[df1["mid"]==1,"Total.Enrollment"]
				mid = mid.reset_index(drop=True)
				
				from sklearn.model_selection import train_test_split as tts
				X_train, X_test, y_train, y_test = tts(mid.loc[:,mid.columns!="y"], mid["y"], test_size=0.2)
			</code> </pre>
			
			
			<h3  id="C1">Model tuning</h3>
			<p style="font-size:18px;">
			Model tuning is the experimental process of finding the optimal values of hyperparameters to maximize model performance. To perform model tuning
				for DT classifiers, I chose 1. middle school and 2. races for the X features data. 
				The maximum depth parameter was iterated from 1 to 20 to train and find the model with the best accuracy. 
			</p>
			
			<h4  id="C1"><center>X-feature: Middle School Enrollments</center></h4>
			
			<figure class="half" style="display:flex">
			    <img src="../images/DT/DT_dataset1_accu1.png" width="500" height="324"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
			    <img src="../images/DT/DT_dataset1_HM1.png" width="400" height="347">
			</figure>
			
			<p style="font-size:18px;">
			The Accuracy vs. max_depth plot shows the accuracy number of predicted label (y=0) of each model with the different max_depth parameter. From
				the plot the prediction labels on the test features have highest accuracy of 0.76 when the parameter is 5, thus using this number the confusion
				matrix of the classification results from Decision Tree is shown on the right.
			</p>
	
			<p style="font-size:18px;">
			From the matrix we can see that the classifier predicted perfectly when the true label is 0, but was extremely poor with true label 1. The  
			plots of the decision tree can be seen below: 
			</p> <br> <br>
			
			<center><img src="../images/DT/DT_dataset1_tree1.png" width="1000" height="800"></center> <br> <br> <br>
			
			<h4  id="C1"><center>X-feature: Enrollments of Different Races</center></h4>
			
			<figure class="half" style="display:flex">
			    <img src="../images/DT/DT_dataset1_accu2.png" width="500" height="324"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
			    <img src="../images/DT/DT_dataset1_HM2.png" width="400" height="347">
			</figure>
			
			<p style="font-size:18px;">
			The Accuracy vs. max_depth plot of this test shows that the prediction labels on the test features have highest accuracy when
				the parameter is 6, thus using this number the confusion matrix of the classification results from Decision Tree is shown on the right.
			</p>
	
			<p style="font-size:18px;">
			From the matrix, again the classifier predicted well when the true label is 0, but was poor with true label 1. When the X-features are the 
				enrollments of different races, the highest accuracy with the hyper parameter is 0.67, which is lower comparing when the X-features are
				enrollments data for middle schools.
			</p> <br> <br>
			
			<h3  id="C1">Conclusion</h3>
			
			<p style="font-size:18px;">
			The Decision Tree model with the X-features of enrollments data of middle schools and y-labels of girls enrollments proportions reports an 
			accuracy of 0.76 after the model tuning process. This indicates that the decision tree model is performing decently, and also there exists 
			a relationship between the different grades and the enrollments difference of boys and girls. For the model used on data with 
			X-features being enrollments data of different races, the accuracy reported is lower, which means that race is not obviously related with the
			enrollments difference of boys and girls.
			</p> <br> <br>
			
			<h4  id="C1"><a href="../code/DT/Decision_Tree.html">Python (jupyter notebook) Code for the Decision Tree Process</a></h4>
		
	</div>

	</body>

</html>
